{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fd169d7-e39a-4077-af6c-ab6f7ccf47b1",
   "metadata": {},
   "source": [
    "# <center> DSC 350 - Week 5 - Exercise 5\n",
    "***\n",
    "## Alana D'Agostino\n",
    "### Professor Kinney\n",
    "Textbook Reference: __[Hands-On Data Analysis with Pandas (2nd Ed.) - Ch. 3](https://github.com/AlanaDAg/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_03)__ <br>\n",
    "Textbook Data Directory: __[Chapter 3 Exercises Data Directory (GitHub)](https://github.com/AlanaDAg/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_03/exercises)__\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f50b9d86-ffe8-42f6-a6a8-ba07d3c817e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code attribution\n",
    "/\n",
    "# ========================================================================================\n",
    "# Title: \"Hands-On Data Analysis with Pandas (Second Edition), Chapter 3\n",
    "# Author: Stefanie Molin\n",
    "# Date: 14 April 2024\n",
    "# Modified By: Alana D'Agostino (DSC 350 - Week 5 - Exercise 5)\n",
    "# Description: This program follows along with the exercises in Chapter 3 of\n",
    "# Stefanie Molin's _Hands-On Data Analysis with Panda (2nd Ed.).\n",
    "# It will perform Data Manipulation including reading in and Combining multiple files\n",
    "## of one type (CSV), performing basic data cleaning tasks, and reshaping DataFrames\n",
    "## with Pandas (melt()).\n",
    "### Data is pulled from the textbook's GitHub Ch 03 directory (under 'exercises').\n",
    "# ========================================================================================\n",
    "/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1ff469-5423-4434-bb99-c53e1ed704b6",
   "metadata": {},
   "source": [
    "# <center> <font color=blue>Chapter 3</font> <br><font color=mediumblue>**Data Wrangling with Pandas**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ca5df2-bb95-4e85-bc3b-9f6299826ddf",
   "metadata": {},
   "source": [
    "***\n",
    "### <center><font color=#00ad43>**Complete the following exercises using what we have learned so far in this book and the data in the `exercises` /directory.**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3bc88e-7b30-44a1-9932-3df98d395015",
   "metadata": {},
   "source": [
    "# <font color=mediumblue>1.</font>**Multiple CSV Files:**<br><center>Reading In and Combining\n",
    "\n",
    "> <font color=deeppink>We want to look at data for the **Facebook, Apple, Amazon, Netflix,** and **Google** (**FAANG**) stocks, but we were given each as a separate CSV file.</font>\n",
    "> > <font color=deeppink>**Combine** the CSV files into a single file\n",
    "> > <font color=deeppink>Then **store** the dataframe of the FAANG data as `faang` for the rest of the exercises:\n",
    "> > > * <font color=deeppink>**Read in** the `aapl.csv`, `amzn.csv`, `fb.csv`, `goog.csv`, and `nflx.csv` files\n",
    "> > > * <font color=deeppink>**Add a column** to each dataframe, called `ticker`, indicating the ticker symbol it is for; this is how you look up a stock. *In this case, the filenames happen to be the ticker symbols.*\n",
    "> > > * <font color=deeppink>**Append** them together into a single dataframe.\n",
    "> > > * <font color=deeppink>**Save** the result in a CSV file called `faang.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36a670ba-3545-46da-a441-8e1cf53887c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FB</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>181.580002</td>\n",
       "      <td>177.550003</td>\n",
       "      <td>177.679993</td>\n",
       "      <td>181.419998</td>\n",
       "      <td>18151900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FB</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>184.779999</td>\n",
       "      <td>181.330002</td>\n",
       "      <td>181.880005</td>\n",
       "      <td>184.669998</td>\n",
       "      <td>16886600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FB</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>186.210007</td>\n",
       "      <td>184.100006</td>\n",
       "      <td>184.899994</td>\n",
       "      <td>184.330002</td>\n",
       "      <td>13880900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FB</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>186.899994</td>\n",
       "      <td>184.929993</td>\n",
       "      <td>185.589996</td>\n",
       "      <td>186.850006</td>\n",
       "      <td>13574500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FB</td>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>188.899994</td>\n",
       "      <td>186.330002</td>\n",
       "      <td>187.199997</td>\n",
       "      <td>188.279999</td>\n",
       "      <td>17994700.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker        date        high         low        open       close  \\\n",
       "0     FB  2018-01-02  181.580002  177.550003  177.679993  181.419998   \n",
       "1     FB  2018-01-03  184.779999  181.330002  181.880005  184.669998   \n",
       "2     FB  2018-01-04  186.210007  184.100006  184.899994  184.330002   \n",
       "3     FB  2018-01-05  186.899994  184.929993  185.589996  186.850006   \n",
       "4     FB  2018-01-08  188.899994  186.330002  187.199997  188.279999   \n",
       "\n",
       "       volume  \n",
       "0  18151900.0  \n",
       "1  16886600.0  \n",
       "2  13880900.0  \n",
       "3  13574500.0  \n",
       "4  17994700.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Combine separate CSV files into a single DataFrame `faang`\n",
    "## Use for-loop to cycle through all the CSV files as a list\n",
    "faang = pd.DataFrame()\n",
    "for ticker in ['fb', 'aapl', 'amzn', 'nflx', 'goog']:\n",
    "    # You can combine raw and f-strings\n",
    "    filepath = (rf'C:\\Users\\alana\\OneDrive\\Desktop\\DSC 350\\Data\\{ticker}.csv')\n",
    "    df_faang = pd.read_csv(filepath)\n",
    "    # Add `ticker` column to dataframes\n",
    "    df_faang.insert(0, 'ticker', ticker.upper())\n",
    "    # .append() was deprecated; Use pd.concat() instead\n",
    "    faang = pd.concat([faang, df_faang])\n",
    "\n",
    "# Save the new combined DataFrame as a CSV file\n",
    "faang.to_csv('faang.csv', index=False)\n",
    "\n",
    "# Inspect the first rows of new faang DataFrame\n",
    "faang.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95844182-545d-4a99-a07b-28536cdd749a",
   "metadata": {},
   "source": [
    "> **Sources:**\n",
    "> * Combining raw and f-string literal: __[Stack Overflow](https://stackoverflow.com/questions/58302531/combine-f-string-and-raw-string-literal)__<br>\n",
    "> * Pandas `.concat()`: __[pydata.org](https://pandas.pydata.org/docs/reference/api/pandas.concat.html)__ & __[GeeksforGeeks](https://www.geeksforgeeks.org/pandas-concat-function-in-python/)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870badbf-f43a-4360-88b0-c961053759ce",
   "metadata": {},
   "source": [
    "***\n",
    "# <font color=mediumblue>2.</font> **Data Cleaning:** <br><center>Data Type Conversions\n",
    "> <font color=deeppink>With `faang`, use **type conversion** to:\n",
    "> > * <font color=deeppink>**Cast** the values of the `date` column into ***datetimes***\n",
    "> > * <font color=deeppink>**Cast** the values of the `volume` column into ***integers***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df506bff-9dc3-4b32-89d8-24782aea9716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature column data types:\n",
      " ticker     object\n",
      "date       object\n",
      "high      float64\n",
      "low       float64\n",
      "open      float64\n",
      "close     float64\n",
      "volume    float64\n",
      "dtype: object\n",
      "\n",
      "Cast `date` column from object to datetimes.\n",
      "Cast `volume` from float64 to integers.\n"
     ]
    }
   ],
   "source": [
    "# Inspect data types for each feature (column)\n",
    "print(\"Feature column data types:\\n\", faang.dtypes)\n",
    "\n",
    "print(\"\\nCast `date` column from object to datetimes.\" + \n",
    "      \"\\nCast `volume` from float64 to integers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f45e2aa0-50e8-45da-a089-09946636522a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>43.075001</td>\n",
       "      <td>42.314999</td>\n",
       "      <td>42.540001</td>\n",
       "      <td>43.064999</td>\n",
       "      <td>102223600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>1190.000000</td>\n",
       "      <td>1170.510010</td>\n",
       "      <td>1172.000000</td>\n",
       "      <td>1189.010010</td>\n",
       "      <td>2694500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FB</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>181.580002</td>\n",
       "      <td>177.550003</td>\n",
       "      <td>177.679993</td>\n",
       "      <td>181.419998</td>\n",
       "      <td>18151900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>1066.939941</td>\n",
       "      <td>1045.229980</td>\n",
       "      <td>1048.339966</td>\n",
       "      <td>1065.000000</td>\n",
       "      <td>1237600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NFLX</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>201.649994</td>\n",
       "      <td>195.419998</td>\n",
       "      <td>196.100006</td>\n",
       "      <td>201.070007</td>\n",
       "      <td>10966900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker       date         high          low         open        close  \\\n",
       "0   AAPL 2018-01-02    43.075001    42.314999    42.540001    43.064999   \n",
       "0   AMZN 2018-01-02  1190.000000  1170.510010  1172.000000  1189.010010   \n",
       "0     FB 2018-01-02   181.580002   177.550003   177.679993   181.419998   \n",
       "0   GOOG 2018-01-02  1066.939941  1045.229980  1048.339966  1065.000000   \n",
       "0   NFLX 2018-01-02   201.649994   195.419998   196.100006   201.070007   \n",
       "\n",
       "      volume  \n",
       "0  102223600  \n",
       "0    2694500  \n",
       "0   18151900  \n",
       "0    1237600  \n",
       "0   10966900  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "faang = faang.assign(\n",
    "    # Lambda functions (Ch.3, pg.144)\n",
    "    # pd.to_datetime() function (Ch.3, pg.141, Molin)\n",
    "    date=lambda x: pd.to_datetime(x.date),\n",
    "    # .astype() method converts a single column at a time (Ch.3,pg.144, Molin)\n",
    "    volume=lambda x: x.volume.astype(int)\n",
    "# Sort the values by `date` and `ticker` features to see first(0) rows of each faang stock\n",
    ").sort_values(['date', 'ticker'])\n",
    "\n",
    "# Inspect the first rows of the dataframe\n",
    "faang.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f820193a-c73a-4d3e-9528-3cadb1106b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types after conversion:\n",
      " ticker            object\n",
      "date      datetime64[ns]\n",
      "high             float64\n",
      "low              float64\n",
      "open             float64\n",
      "close            float64\n",
      "volume             int32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Confirm the data type conversions\n",
    "print(\"Data types after conversion:\\n\", faang.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f156d9-07b3-4e40-8fb3-340f047d8336",
   "metadata": {},
   "source": [
    "> **NOTE:** Type conversion (Ch.3, pgs. 140-146, Molin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fd28ad-fc79-4b1e-ad9e-7b39dd87401b",
   "metadata": {},
   "source": [
    "***\n",
    "# <font color=mediumblue>3.</font> **Sorting:** <br><center>Subsets of Values\n",
    "> <font color=deeppink>Find the **seven** (**7**) rows in `faang` with the lowest value for `volume`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb2aa711-2e64-44c8-b403-6beaf537e2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>2018-07-03</td>\n",
       "      <td>1135.819946</td>\n",
       "      <td>1100.020020</td>\n",
       "      <td>1135.819946</td>\n",
       "      <td>1102.890015</td>\n",
       "      <td>679000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>2018-11-23</td>\n",
       "      <td>1037.589966</td>\n",
       "      <td>1022.398987</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1023.880005</td>\n",
       "      <td>691500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>2018-05-24</td>\n",
       "      <td>1080.469971</td>\n",
       "      <td>1066.150024</td>\n",
       "      <td>1079.000000</td>\n",
       "      <td>1079.239990</td>\n",
       "      <td>766800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>2018-07-10</td>\n",
       "      <td>1159.589966</td>\n",
       "      <td>1149.589966</td>\n",
       "      <td>1156.979980</td>\n",
       "      <td>1152.839966</td>\n",
       "      <td>798400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>2018-08-09</td>\n",
       "      <td>1255.541992</td>\n",
       "      <td>1246.010010</td>\n",
       "      <td>1249.900024</td>\n",
       "      <td>1249.099976</td>\n",
       "      <td>848600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>2018-08-20</td>\n",
       "      <td>1211.000000</td>\n",
       "      <td>1194.625977</td>\n",
       "      <td>1205.020020</td>\n",
       "      <td>1207.770020</td>\n",
       "      <td>870800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>2018-08-22</td>\n",
       "      <td>1211.839966</td>\n",
       "      <td>1199.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1207.329956</td>\n",
       "      <td>887400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ticker       date         high          low         open        close  \\\n",
       "126   GOOG 2018-07-03  1135.819946  1100.020020  1135.819946  1102.890015   \n",
       "226   GOOG 2018-11-23  1037.589966  1022.398987  1030.000000  1023.880005   \n",
       "99    GOOG 2018-05-24  1080.469971  1066.150024  1079.000000  1079.239990   \n",
       "130   GOOG 2018-07-10  1159.589966  1149.589966  1156.979980  1152.839966   \n",
       "152   GOOG 2018-08-09  1255.541992  1246.010010  1249.900024  1249.099976   \n",
       "159   GOOG 2018-08-20  1211.000000  1194.625977  1205.020020  1207.770020   \n",
       "161   GOOG 2018-08-22  1211.839966  1199.000000  1200.000000  1207.329956   \n",
       "\n",
       "     volume  \n",
       "126  679000  \n",
       "226  691500  \n",
       "99   766800  \n",
       "130  798400  \n",
       "152  848600  \n",
       "159  870800  \n",
       "161  887400  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use nsmallest() method to grab n smallest rows (Ch.3, pg.148, Molin)\n",
    "faang.nsmallest(7, 'volume')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a966afc-566f-4bd0-af48-1a41e52d57c1",
   "metadata": {},
   "source": [
    "> **NOTE:** The `nsmallest()` method returns a dictated number of rows (*n* argument), after sorting the DataFrame from smallest to largest values for a specified column.\n",
    "> > Places the smallest values at the top of the DF.\n",
    "\n",
    "> The `nlargest()` method performs a similar manipulation, only it sorts the DF by the largest to smallest value for a specified column.\n",
    "\n",
    "> **Syntax:** <center>`nsmallest()` = *dataframe*.nsmallest(*n*, *columns*, keep)<br>\n",
    "`nlargest()` = *dataframe*.nlargest(*n*, *columns*, keep)\n",
    "\n",
    "**Sources:**\n",
    "* __[nsmallest()](https://www.w3schools.com/python/pandas/ref_df_nsmallest.asp)__ & __[nlargest()](https://www.w3schools.com/python/pandas/ref_df_nlargest.asp)__ - W3Schools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de26109b-de18-4987-afb3-4572f37e917b",
   "metadata": {},
   "source": [
    "***\n",
    "# <font color=mediumblue>4.</font> **Reshaping Data:** Melting DataFrames <br><center>Long and Wide Formats\n",
    "> <font color=deeppink>Right now, the data is somewhere between long and wide format.\n",
    "> > <font color=deeppink>Use `melt()` to make it completely long format.\n",
    "\n",
    "<font color=black><center>**HINT:** <font color=cherryblossom> &diams;&diams;`date` and `ticker` are our ID variables (they uniquely identify each row).</center>\n",
    "\n",
    "> > <font color=deeppink>We need to melt the rest so that we don't have separate columns for `open`, `high`, `low`, `close`, and `volume`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cee9b9c-81d3-4eb5-bad0-7d57cf8904f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>open</td>\n",
       "      <td>42.540001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>open</td>\n",
       "      <td>1172.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FB</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>open</td>\n",
       "      <td>177.679993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>open</td>\n",
       "      <td>1048.339966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NFLX</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>open</td>\n",
       "      <td>196.100006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>open</td>\n",
       "      <td>43.132500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>open</td>\n",
       "      <td>1188.300049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FB</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>open</td>\n",
       "      <td>181.880005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker       date variable        value\n",
       "0   AAPL 2018-01-02     open    42.540001\n",
       "1   AMZN 2018-01-02     open  1172.000000\n",
       "2     FB 2018-01-02     open   177.679993\n",
       "3   GOOG 2018-01-02     open  1048.339966\n",
       "4   NFLX 2018-01-02     open   196.100006\n",
       "5   AAPL 2018-01-03     open    43.132500\n",
       "6   AMZN 2018-01-03     open  1188.300049\n",
       "7     FB 2018-01-03     open   181.880005"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Melting tranforms data frome wide to long format\n",
    "## .melt() unpivots a DataFrame from wide to long (undoes a pivot)\n",
    "melted_faang = faang.melt(\n",
    "    # id_vars and value_vars arguments = lists\n",
    "    id_vars=['ticker', 'date'],\n",
    "    value_vars=['open', 'high', 'low', 'close', 'volume'])\n",
    "\n",
    "# There will now be two new columns (`variable` and `value`) that will replace\n",
    "## the `open`, `high`, `low`, `close`, and `volume` columns\n",
    "\n",
    "# Inspect the first rows of the melted DataFrame\n",
    "melted_faang.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b869fd39-287f-4103-9b17-ed364d1d2e5f",
   "metadata": {},
   "source": [
    "> **NOTE:** The `.melt()` function massages a DataFrame where one or more feature columns are identifier (ID) variables, while all other columns (measured variables (value_vars)) are unpivoted to the row axis, which leaves two(2) non-ID columns - 'variable' and 'value'.\n",
    "> > Arguments of interest: <br> **id_vars**: *scalar, tuple, list, or ndarray* - optional <br> **value_vars**: *scalar, tuple, list, or ndarray* - optional\n",
    "\n",
    "> **Note:** Melting DataFrames (Ch.3, pgs.169-172, Molin) <br>\n",
    "> **Source:** Pandas `.melt()`: __[pydata.org](https://pandas.pydata.org/docs/reference/api/pandas.melt.html)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfddf1b-e8dd-46a6-8b41-3f846240ef2c",
   "metadata": {},
   "source": [
    "***\n",
    "# <font color=mediumblue>5.</font> **Handling Data:** <br><center>Duplicate, Missing, or **Invalid** Data\n",
    "> <font color=deeppink>Suppose we found out that on July 26, 2018, there was a glitch in how the data was recorded.\n",
    "> > <font color=deeppink>**How should we handle this?**\n",
    "<font color=black><center>**NOTE:** <font color=cherryblossom>There is no coding required for this exercise.</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06c5a935-9de8-4805-9de0-a03adc697349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are several ways to handle invalid data and the best option is the one that has the least negative impact on the quality of the overall dataset. It is important to remember that dropping even a few rows from a large dataset can make a significant impact, depending on the context and data source. The same impact can result from interpolating values in a few rows, as well. \n",
      "We should consider the number of invalid values, their locations within the dataset, and any other potential sources to locate the correct values. Using the `fillna()` method here is one of the better options.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are several ways to handle invalid data and the best option is the one \" +\n",
    "      \"that has the least negative impact on the quality of the overall dataset. \" +\n",
    "      \"It is important to remember that dropping even a few rows from a large dataset \" +\n",
    "      \"can make a significant impact, depending on the context and data source. \" +\n",
    "      \"The same impact can result from interpolating values in a few rows, as well. \" +\n",
    "      \"\\nWe should consider the number of invalid values, their locations within the \" +\n",
    "      \"dataset, and any other potential sources to locate the correct values. \" +\n",
    "      \"Using the `fillna()` method here is one of the better options.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc29ca41-0b27-4982-9d79-99b0a2bf3de7",
   "metadata": {},
   "source": [
    "> **NOTE:** Handling duplicate, missing, or invalid data (Ch.3, pgs.172-188, Molin) <br>Mitigating the Issues (pgs.179-188)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
