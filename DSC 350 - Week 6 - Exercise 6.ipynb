{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d506b1d-bb59-410c-8fa8-72dc5b325f01",
   "metadata": {},
   "source": [
    "# <center> DSC 350 - Week 6 - Exercise 6\n",
    "***\n",
    "## Alana D'Agostino\n",
    "### Professor Kinney\n",
    "Textbook Reference: __[Hands-On Data Analysis with Pandas (2nd Ed.) - Ch. 4](https://github.com/AlanaDAg/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_04)__ <br>\n",
    "Textbook Data Directory: __[Chapter 4 Exercises Data Directory (GitHub)](https://github.com/AlanaDAg/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_04/exercises)__\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d773fa6b-9da5-4398-ad45-a71d2bd951c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code attribution\n",
    "/\n",
    "# ========================================================================================\n",
    "# Title: \"Hands-On Data Analysis with Pandas (Second Edition), Chapter 4\n",
    "# Author: Stefanie Molin\n",
    "# Date: 21 April 2024\n",
    "# Modified By: Alana D'Agostino (DSC 350 - Week 6 - Exercise 6)\n",
    "# Description: This program follows along with the exercises (1-9) of Chapter 4 in Stefanie Molin's \"Hands-On\" textbook.\n",
    "## It practices aggregating Pandas DataFrames, performing calculations and DataFrames manipulations, as well as joining DataFrames.\n",
    "# ========================================================================================\n",
    "/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1990c99b-4d28-4b4c-9799-923fff06ab30",
   "metadata": {},
   "source": [
    "# <center><font color=blue>Chapter 4</font> <br><font color=mediumblue>**Aggregating Pandas DataFrames**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c538a0-155f-4e47-a0d3-c40e3e7e874b",
   "metadata": {},
   "source": [
    "***\n",
    "### <center><font color=#00ad43>**Using the CSV files in the `exercises/` folder and what we have learned so far in this book, complete the following exercises.**\n",
    "***\n",
    "#### <center><font color=darkgreen>**Page 257, Exercises 1-9**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8565eecb-2ec5-4df9-8915-df040df87acf",
   "metadata": {},
   "source": [
    "***\n",
    "# <font color=blue>**1.**</font> **Indexing & Slicing Data:** <center>Selecting Data by Specific Conditions\n",
    "\n",
    "> <font color=deeppink>With the `earthquakes.csv` file, select all the earthquakes in **Japan** with a magnitude of **4.9 or greater** using the `mb` magnitude type.\n",
    ">> <font color=purple>**NOTE:** Greater than or equal to 4.9; use >= operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e27899c1-f00f-4d7f-952f-b49c2791f644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mag</th>\n",
       "      <th>magType</th>\n",
       "      <th>time</th>\n",
       "      <th>place</th>\n",
       "      <th>tsunami</th>\n",
       "      <th>parsed_place</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.35</td>\n",
       "      <td>ml</td>\n",
       "      <td>1539475168010</td>\n",
       "      <td>9km NE of Aguanga, CA</td>\n",
       "      <td>0</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.29</td>\n",
       "      <td>ml</td>\n",
       "      <td>1539475129610</td>\n",
       "      <td>9km NE of Aguanga, CA</td>\n",
       "      <td>0</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.42</td>\n",
       "      <td>ml</td>\n",
       "      <td>1539475062610</td>\n",
       "      <td>8km NE of Aguanga, CA</td>\n",
       "      <td>0</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.44</td>\n",
       "      <td>ml</td>\n",
       "      <td>1539474978070</td>\n",
       "      <td>9km NE of Aguanga, CA</td>\n",
       "      <td>0</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.16</td>\n",
       "      <td>md</td>\n",
       "      <td>1539474716050</td>\n",
       "      <td>10km NW of Avenal, CA</td>\n",
       "      <td>0</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mag magType           time                  place  tsunami parsed_place\n",
       "0  1.35      ml  1539475168010  9km NE of Aguanga, CA        0   California\n",
       "1  1.29      ml  1539475129610  9km NE of Aguanga, CA        0   California\n",
       "2  3.42      ml  1539475062610  8km NE of Aguanga, CA        0   California\n",
       "3  0.44      ml  1539474978070  9km NE of Aguanga, CA        0   California\n",
       "4  2.16      md  1539474716050  10km NW of Avenal, CA        0   California"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare Notebook\n",
    "## Import libraries and read in data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "earthquakes = pd.read_csv('https://github.com/AlanaDAg/'\n",
    "                          'Hands-On-Data-Analysis-with-Pandas-2nd-edition/'\n",
    "                          'blob/master/ch_04/exercises/earthquakes.csv?raw=True')\n",
    "earthquakes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b48a162-ca0b-4622-9b33-f3254606cabe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mag</th>\n",
       "      <th>magType</th>\n",
       "      <th>time</th>\n",
       "      <th>place</th>\n",
       "      <th>tsunami</th>\n",
       "      <th>parsed_place</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>4.9</td>\n",
       "      <td>mb</td>\n",
       "      <td>1538977532250</td>\n",
       "      <td>293km ESE of Iwo Jima, Japan</td>\n",
       "      <td>0</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2576</th>\n",
       "      <td>5.4</td>\n",
       "      <td>mb</td>\n",
       "      <td>1538697528010</td>\n",
       "      <td>37km E of Tomakomai, Japan</td>\n",
       "      <td>0</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3072</th>\n",
       "      <td>4.9</td>\n",
       "      <td>mb</td>\n",
       "      <td>1538579732490</td>\n",
       "      <td>15km ENE of Hasaki, Japan</td>\n",
       "      <td>0</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3632</th>\n",
       "      <td>4.9</td>\n",
       "      <td>mb</td>\n",
       "      <td>1538450871260</td>\n",
       "      <td>53km ESE of Hitachi, Japan</td>\n",
       "      <td>0</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mag magType           time                         place  tsunami  \\\n",
       "1563  4.9      mb  1538977532250  293km ESE of Iwo Jima, Japan        0   \n",
       "2576  5.4      mb  1538697528010    37km E of Tomakomai, Japan        0   \n",
       "3072  4.9      mb  1538579732490     15km ENE of Hasaki, Japan        0   \n",
       "3632  4.9      mb  1538450871260    53km ESE of Hitachi, Japan        0   \n",
       "\n",
       "     parsed_place  \n",
       "1563        Japan  \n",
       "2576        Japan  \n",
       "3072        Japan  \n",
       "3632        Japan  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select all the earthquakes in Japan with a magnitude of 4.9 or greater\n",
    "## Use the `mb` magnitude type (`magType' column)\n",
    "\n",
    "# Use .loc[] indexer to select specific rows and/or columns when using row and column names\n",
    "\n",
    "#Japan_quakes = display(earthquakes.loc[\n",
    "                      # (earthquakes.parsed_place == 'Japan') &\n",
    "                      # (earthquakes.mag >= 4.9) &\n",
    "                      # (earthquakes.magType == 'mb')\n",
    "                      # ])\n",
    "\n",
    "#Japan_quakes\n",
    "\n",
    "# ***** Here is another way to do it that uses .query() method\n",
    "## .query() always returns a copy of the original DataFrame and NEVER changes the original DF\n",
    "\n",
    "Japan_quakesTWO = earthquakes.query(\"parsed_place == 'Japan' and \"\n",
    "                                    \"magType == 'mb' and \"\n",
    "                                    \"mag >= 4.9\"\n",
    "                                   )\n",
    "Japan_quakesTWO                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a63603e-4263-4712-ab6f-a44958b0df17",
   "metadata": {},
   "source": [
    "### **Sources:**\n",
    "\n",
    "> **Supplemental:**\n",
    "* pandas.pydata: __[How do I select a subset of a `DataFrame`?](https://pandas.pydata.org/docs/getting_started/intro_tutorials/03_subset_data.html)__ <br>\n",
    "* GeeksforGeeks: __[Difference between loc() and iloc() in Pandas DataFrame](https://www.geeksforgeeks.org/difference-between-loc-and-iloc-in-pandas-dataframe/)__ <br>\n",
    "* Towards Data Science: __[A Python Beginner's Look at .loc](https://towardsdatascience.com/a-python-beginners-look-at-loc-part-1-cb1e1e565ec2)__<br>\n",
    "* Towards Data Science: __[How to use loc and iloc for selecting data in Pandas](https://towardsdatascience.com/how-to-use-loc-and-iloc-for-selecting-data-in-pandas-bd09cb4c3d79)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509874a5-566c-4c5e-8986-8ea525f384fa",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# <font color=blue>**2.**</font> **Enriching Data with DF Operations:** <center> Binning\n",
    "\n",
    "> <font color=deeppink> **Create bins** for each full number of earthquake magnitude with the `m1` magnitude type and count how many are in each bin.\n",
    "> > <font color=darkpink>For instance, the first bin is</font> (0, 1], <font color=darkpink>the second is</font> (1, 2], <font color=darkpink>and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93812f7-363e-49fa-a2e5-7c2889d9e42e",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">**NOTES** </span> <center>Ch. 4, pp. 213-217\n",
    "    \n",
    "* <font color=darkviolet>Binning aka Discretizing (going from continuous to discrete)\n",
    "  > Taking data and placing the observations into bins that match the range they fall into.\n",
    "  > > Doing this reduces the number of distinct values our data can take on and make it easier to analyze.\n",
    "  > > > Because, sometimes, it is more convenient to work with categories instead of specific values.\n",
    "* <font color=darkviolet>Binning reduces granularity and, therefore, the information in that field is also reduced.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfe1ac97-0ac9-4330-86f6-9d1471b96683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mag_bin\n",
       "(1, 2]    3105\n",
       "(0, 1]    2207\n",
       "(2, 3]     862\n",
       "(3, 4]     122\n",
       "(4, 5]       2\n",
       "(5, 6]       1\n",
       "(6, 7]       0\n",
       "(7, 8]       0\n",
       "(8, 9]       0\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bin by each FULL number of magnitude (aka create ranges of magnitude)\n",
    "## Takes the continuous magnitude data and bins it into categories defined by magnitude range\n",
    "\n",
    "earthquakes.query(\"magType == 'ml'\").assign(\n",
    "    mag_bin=lambda x: pd.cut(x.mag, np.arange(0, 10))\n",
    ").mag_bin.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd2467a-ad77-4640-a5fd-400655ffcca3",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# <font color=blue>**3.**</font> **Aggregating Data:** <center> Aggregating by Group\n",
    "\n",
    "> <font color=deeppink>Using the `faang.csv` file:\n",
    "> > <font color=deeppink>**Group** by the</font> ***ticker*** <br>\n",
    "> and <br>\n",
    "> > <font color=deeppink>**Resample** to</font> ***monthly frequency***\n",
    "> \n",
    ">  <font color=deeppink>Then, make the following **aggregations:**\n",
    "> > * <font color=deeppink>**Mean** of the opening price\n",
    "> > * <font color=deeppink>**Maximum** of the high price\n",
    "> > * <font color=deeppink>**Minimum** of the low price\n",
    "> > * <font color=deeppink>**Mean** of the closing price\n",
    "> > * <font color=deeppink>**Sum** of the volume traded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e4e53c5-cea4-4a89-88e2-93686e4dd086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1255 entries, 0 to 1254\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   ticker  1255 non-null   object \n",
      " 1   date    1255 non-null   object \n",
      " 2   high    1255 non-null   float64\n",
      " 3   low     1255 non-null   float64\n",
      " 4   open    1255 non-null   float64\n",
      " 5   close   1255 non-null   float64\n",
      " 6   volume  1255 non-null   float64\n",
      "dtypes: float64(5), object(2)\n",
      "memory usage: 68.8+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FB</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>181.580002</td>\n",
       "      <td>177.550003</td>\n",
       "      <td>177.679993</td>\n",
       "      <td>181.419998</td>\n",
       "      <td>18151900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FB</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>184.779999</td>\n",
       "      <td>181.330002</td>\n",
       "      <td>181.880005</td>\n",
       "      <td>184.669998</td>\n",
       "      <td>16886600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FB</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>186.210007</td>\n",
       "      <td>184.100006</td>\n",
       "      <td>184.899994</td>\n",
       "      <td>184.330002</td>\n",
       "      <td>13880900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FB</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>186.899994</td>\n",
       "      <td>184.929993</td>\n",
       "      <td>185.589996</td>\n",
       "      <td>186.850006</td>\n",
       "      <td>13574500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FB</td>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>188.899994</td>\n",
       "      <td>186.330002</td>\n",
       "      <td>187.199997</td>\n",
       "      <td>188.279999</td>\n",
       "      <td>17994700.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker        date        high         low        open       close  \\\n",
       "0     FB  2018-01-02  181.580002  177.550003  177.679993  181.419998   \n",
       "1     FB  2018-01-03  184.779999  181.330002  181.880005  184.669998   \n",
       "2     FB  2018-01-04  186.210007  184.100006  184.899994  184.330002   \n",
       "3     FB  2018-01-05  186.899994  184.929993  185.589996  186.850006   \n",
       "4     FB  2018-01-08  188.899994  186.330002  187.199997  188.279999   \n",
       "\n",
       "       volume  \n",
       "0  18151900.0  \n",
       "1  16886600.0  \n",
       "2  13880900.0  \n",
       "3  13574500.0  \n",
       "4  17994700.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "faang = pd.read_csv('https://github.com/AlanaDAg/'\n",
    "                    'Hands-On-Data-Analysis-with-Pandas-2nd-edition/'\n",
    "                    'blob/master/ch_04/exercises/faang.csv?raw=True')\n",
    "\n",
    "# Examine data types and head of DataFrame\n",
    "print(faang.info())\n",
    "faang.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7b2ca5-6a2d-47da-a2fc-cc2f3c7bcf6f",
   "metadata": {},
   "source": [
    "**NOTE:** From pandas.pydata on `pandas.read_csv()` parameter: <br><center>\n",
    "**date_format** : *str or dict of column -> format, default None*\n",
    "\n",
    "> If used in conjunction with **parse_dates**, will parse dates according to this format.<br> \n",
    "   For anything more complex, please read in as *object* and then apply **to_datetime()** as-needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50a8a96a-a6f3-47e8-a5f2-911d5bc4217a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1255 entries, 0 to 1254\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Non-Null Count  Dtype         \n",
      "---  ------  --------------  -----         \n",
      " 0   ticker  1255 non-null   object        \n",
      " 1   date    1255 non-null   datetime64[ns]\n",
      " 2   high    1255 non-null   float64       \n",
      " 3   low     1255 non-null   float64       \n",
      " 4   open    1255 non-null   float64       \n",
      " 5   close   1255 non-null   float64       \n",
      " 6   volume  1255 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(5), object(1)\n",
      "memory usage: 68.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Convert 'date' column from object data type to datetime data type\n",
    "faang['date'] = pd.to_datetime(faang['date'])\n",
    "\n",
    "# Check that data type conversion worked\n",
    "faang.info()\n",
    "\n",
    "# stackoverflow post and Towards Data Science article helped me fix the ParserError\n",
    "## with trying to parse object data with a DateTimeIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb815efe-5dde-4ad5-8e0d-433c4b6a5294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">AAPL</th>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>43.505357</td>\n",
       "      <td>45.025002</td>\n",
       "      <td>41.174999</td>\n",
       "      <td>43.501309</td>\n",
       "      <td>2.638718e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-28</th>\n",
       "      <td>41.819079</td>\n",
       "      <td>45.154999</td>\n",
       "      <td>37.560001</td>\n",
       "      <td>41.909737</td>\n",
       "      <td>3.711577e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-31</th>\n",
       "      <td>43.761786</td>\n",
       "      <td>45.875000</td>\n",
       "      <td>41.235001</td>\n",
       "      <td>43.624048</td>\n",
       "      <td>2.854911e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-30</th>\n",
       "      <td>42.441310</td>\n",
       "      <td>44.735001</td>\n",
       "      <td>40.157501</td>\n",
       "      <td>42.458572</td>\n",
       "      <td>2.664617e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>46.239091</td>\n",
       "      <td>47.592499</td>\n",
       "      <td>41.317501</td>\n",
       "      <td>46.384205</td>\n",
       "      <td>2.483905e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-30</th>\n",
       "      <td>47.180119</td>\n",
       "      <td>48.549999</td>\n",
       "      <td>45.182499</td>\n",
       "      <td>47.155357</td>\n",
       "      <td>2.110498e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31</th>\n",
       "      <td>47.549048</td>\n",
       "      <td>48.990002</td>\n",
       "      <td>45.855000</td>\n",
       "      <td>47.577857</td>\n",
       "      <td>1.574766e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31</th>\n",
       "      <td>53.121739</td>\n",
       "      <td>57.217499</td>\n",
       "      <td>49.327499</td>\n",
       "      <td>53.336522</td>\n",
       "      <td>2.801276e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-30</th>\n",
       "      <td>55.582763</td>\n",
       "      <td>57.417500</td>\n",
       "      <td>53.825001</td>\n",
       "      <td>55.518421</td>\n",
       "      <td>2.715888e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-31</th>\n",
       "      <td>55.300000</td>\n",
       "      <td>58.367500</td>\n",
       "      <td>51.522499</td>\n",
       "      <td>55.211413</td>\n",
       "      <td>3.158994e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-30</th>\n",
       "      <td>47.954881</td>\n",
       "      <td>55.590000</td>\n",
       "      <td>42.564999</td>\n",
       "      <td>47.808929</td>\n",
       "      <td>3.845306e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>41.310789</td>\n",
       "      <td>46.235001</td>\n",
       "      <td>36.647499</td>\n",
       "      <td>41.066579</td>\n",
       "      <td>3.595690e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">AMZN</th>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>1301.377151</td>\n",
       "      <td>1472.579956</td>\n",
       "      <td>1170.510010</td>\n",
       "      <td>1309.010946</td>\n",
       "      <td>9.637120e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-28</th>\n",
       "      <td>1447.113159</td>\n",
       "      <td>1528.699951</td>\n",
       "      <td>1265.930054</td>\n",
       "      <td>1442.363146</td>\n",
       "      <td>1.377840e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-31</th>\n",
       "      <td>1542.160464</td>\n",
       "      <td>1617.540039</td>\n",
       "      <td>1365.199951</td>\n",
       "      <td>1540.367629</td>\n",
       "      <td>1.304001e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-30</th>\n",
       "      <td>1475.841902</td>\n",
       "      <td>1638.099976</td>\n",
       "      <td>1352.880005</td>\n",
       "      <td>1468.220471</td>\n",
       "      <td>1.299196e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>1590.474543</td>\n",
       "      <td>1635.000000</td>\n",
       "      <td>1546.020020</td>\n",
       "      <td>1594.903637</td>\n",
       "      <td>7.161550e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-30</th>\n",
       "      <td>1699.088582</td>\n",
       "      <td>1763.099976</td>\n",
       "      <td>1635.089966</td>\n",
       "      <td>1698.823812</td>\n",
       "      <td>8.594130e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31</th>\n",
       "      <td>1786.305716</td>\n",
       "      <td>1880.050049</td>\n",
       "      <td>1678.060059</td>\n",
       "      <td>1784.649042</td>\n",
       "      <td>9.752110e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31</th>\n",
       "      <td>1891.957833</td>\n",
       "      <td>2025.569946</td>\n",
       "      <td>1776.020020</td>\n",
       "      <td>1897.851308</td>\n",
       "      <td>9.657580e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-30</th>\n",
       "      <td>1969.239476</td>\n",
       "      <td>2050.500000</td>\n",
       "      <td>1865.000000</td>\n",
       "      <td>1966.077900</td>\n",
       "      <td>9.444550e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-31</th>\n",
       "      <td>1799.630865</td>\n",
       "      <td>2033.189941</td>\n",
       "      <td>1476.359985</td>\n",
       "      <td>1782.058265</td>\n",
       "      <td>1.832208e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-30</th>\n",
       "      <td>1622.323806</td>\n",
       "      <td>1784.000000</td>\n",
       "      <td>1420.000000</td>\n",
       "      <td>1625.483823</td>\n",
       "      <td>1.392900e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>1572.922100</td>\n",
       "      <td>1778.339966</td>\n",
       "      <td>1307.000000</td>\n",
       "      <td>1559.443154</td>\n",
       "      <td>1.548127e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">FB</th>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>184.584284</td>\n",
       "      <td>190.660004</td>\n",
       "      <td>175.800003</td>\n",
       "      <td>184.962856</td>\n",
       "      <td>4.956557e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-28</th>\n",
       "      <td>180.721578</td>\n",
       "      <td>195.320007</td>\n",
       "      <td>167.179993</td>\n",
       "      <td>180.269473</td>\n",
       "      <td>5.162516e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-31</th>\n",
       "      <td>173.449524</td>\n",
       "      <td>186.100006</td>\n",
       "      <td>149.020004</td>\n",
       "      <td>173.489522</td>\n",
       "      <td>9.962017e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-30</th>\n",
       "      <td>164.163332</td>\n",
       "      <td>177.100006</td>\n",
       "      <td>150.509995</td>\n",
       "      <td>163.810476</td>\n",
       "      <td>7.500727e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>181.910909</td>\n",
       "      <td>192.720001</td>\n",
       "      <td>170.229996</td>\n",
       "      <td>182.930000</td>\n",
       "      <td>4.011441e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-30</th>\n",
       "      <td>194.974763</td>\n",
       "      <td>203.550003</td>\n",
       "      <td>186.429993</td>\n",
       "      <td>195.267620</td>\n",
       "      <td>3.872656e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31</th>\n",
       "      <td>199.332381</td>\n",
       "      <td>218.619995</td>\n",
       "      <td>166.559998</td>\n",
       "      <td>199.967142</td>\n",
       "      <td>6.470307e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31</th>\n",
       "      <td>177.598695</td>\n",
       "      <td>188.300003</td>\n",
       "      <td>170.270004</td>\n",
       "      <td>177.492172</td>\n",
       "      <td>5.488327e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-30</th>\n",
       "      <td>164.233158</td>\n",
       "      <td>173.889999</td>\n",
       "      <td>158.869995</td>\n",
       "      <td>164.377368</td>\n",
       "      <td>5.004688e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-31</th>\n",
       "      <td>154.873479</td>\n",
       "      <td>165.880005</td>\n",
       "      <td>139.029999</td>\n",
       "      <td>154.187826</td>\n",
       "      <td>6.224463e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-30</th>\n",
       "      <td>141.762857</td>\n",
       "      <td>154.130005</td>\n",
       "      <td>126.849998</td>\n",
       "      <td>141.635715</td>\n",
       "      <td>5.181517e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>137.529475</td>\n",
       "      <td>147.190002</td>\n",
       "      <td>123.019997</td>\n",
       "      <td>137.161052</td>\n",
       "      <td>5.587862e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">GOOG</th>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>1127.200945</td>\n",
       "      <td>1186.890015</td>\n",
       "      <td>1045.229980</td>\n",
       "      <td>1130.770467</td>\n",
       "      <td>2.873840e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-28</th>\n",
       "      <td>1088.629472</td>\n",
       "      <td>1174.000000</td>\n",
       "      <td>992.559998</td>\n",
       "      <td>1088.206839</td>\n",
       "      <td>4.238200e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-31</th>\n",
       "      <td>1096.108085</td>\n",
       "      <td>1177.050049</td>\n",
       "      <td>980.640015</td>\n",
       "      <td>1091.490479</td>\n",
       "      <td>4.535330e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-30</th>\n",
       "      <td>1038.415237</td>\n",
       "      <td>1094.165039</td>\n",
       "      <td>990.369995</td>\n",
       "      <td>1035.696187</td>\n",
       "      <td>4.171590e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>1064.021376</td>\n",
       "      <td>1110.750000</td>\n",
       "      <td>1006.289978</td>\n",
       "      <td>1069.275901</td>\n",
       "      <td>3.184940e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-30</th>\n",
       "      <td>1136.396182</td>\n",
       "      <td>1186.286011</td>\n",
       "      <td>1096.010010</td>\n",
       "      <td>1137.626668</td>\n",
       "      <td>3.209600e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31</th>\n",
       "      <td>1183.464280</td>\n",
       "      <td>1273.890015</td>\n",
       "      <td>1093.800049</td>\n",
       "      <td>1187.590472</td>\n",
       "      <td>3.194010e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31</th>\n",
       "      <td>1226.156951</td>\n",
       "      <td>1256.500000</td>\n",
       "      <td>1188.239990</td>\n",
       "      <td>1225.671732</td>\n",
       "      <td>2.880840e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-30</th>\n",
       "      <td>1176.878424</td>\n",
       "      <td>1212.989990</td>\n",
       "      <td>1146.910034</td>\n",
       "      <td>1175.808934</td>\n",
       "      <td>2.886240e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-31</th>\n",
       "      <td>1116.082172</td>\n",
       "      <td>1209.959961</td>\n",
       "      <td>995.830017</td>\n",
       "      <td>1110.940411</td>\n",
       "      <td>4.849470e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-30</th>\n",
       "      <td>1054.971424</td>\n",
       "      <td>1095.569946</td>\n",
       "      <td>996.020020</td>\n",
       "      <td>1056.162394</td>\n",
       "      <td>3.673510e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>1042.619998</td>\n",
       "      <td>1124.650024</td>\n",
       "      <td>970.109985</td>\n",
       "      <td>1037.420519</td>\n",
       "      <td>4.025760e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">NFLX</th>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>231.269525</td>\n",
       "      <td>286.809998</td>\n",
       "      <td>195.419998</td>\n",
       "      <td>232.908096</td>\n",
       "      <td>2.383776e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-28</th>\n",
       "      <td>270.873158</td>\n",
       "      <td>297.359985</td>\n",
       "      <td>236.110001</td>\n",
       "      <td>271.443683</td>\n",
       "      <td>1.845858e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-31</th>\n",
       "      <td>312.712859</td>\n",
       "      <td>333.980011</td>\n",
       "      <td>275.899994</td>\n",
       "      <td>312.228097</td>\n",
       "      <td>2.634494e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-30</th>\n",
       "      <td>309.129524</td>\n",
       "      <td>338.820007</td>\n",
       "      <td>271.220001</td>\n",
       "      <td>307.466192</td>\n",
       "      <td>2.620060e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>329.779541</td>\n",
       "      <td>356.100006</td>\n",
       "      <td>305.730011</td>\n",
       "      <td>331.536819</td>\n",
       "      <td>1.420508e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-30</th>\n",
       "      <td>384.557143</td>\n",
       "      <td>423.209991</td>\n",
       "      <td>352.820007</td>\n",
       "      <td>384.133336</td>\n",
       "      <td>2.440318e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31</th>\n",
       "      <td>380.969526</td>\n",
       "      <td>419.769989</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>381.515238</td>\n",
       "      <td>3.053938e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31</th>\n",
       "      <td>345.410001</td>\n",
       "      <td>376.809998</td>\n",
       "      <td>310.929993</td>\n",
       "      <td>346.257824</td>\n",
       "      <td>2.131223e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-30</th>\n",
       "      <td>363.326843</td>\n",
       "      <td>383.200012</td>\n",
       "      <td>335.829987</td>\n",
       "      <td>362.641576</td>\n",
       "      <td>1.708321e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-31</th>\n",
       "      <td>340.025218</td>\n",
       "      <td>386.799988</td>\n",
       "      <td>271.209991</td>\n",
       "      <td>335.445652</td>\n",
       "      <td>3.635898e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-30</th>\n",
       "      <td>290.643335</td>\n",
       "      <td>332.049988</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>290.344764</td>\n",
       "      <td>2.571264e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>266.309474</td>\n",
       "      <td>298.720001</td>\n",
       "      <td>231.229996</td>\n",
       "      <td>265.302630</td>\n",
       "      <td>2.343100e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          open         high          low        close  \\\n",
       "ticker date                                                             \n",
       "AAPL   2018-01-31    43.505357    45.025002    41.174999    43.501309   \n",
       "       2018-02-28    41.819079    45.154999    37.560001    41.909737   \n",
       "       2018-03-31    43.761786    45.875000    41.235001    43.624048   \n",
       "       2018-04-30    42.441310    44.735001    40.157501    42.458572   \n",
       "       2018-05-31    46.239091    47.592499    41.317501    46.384205   \n",
       "       2018-06-30    47.180119    48.549999    45.182499    47.155357   \n",
       "       2018-07-31    47.549048    48.990002    45.855000    47.577857   \n",
       "       2018-08-31    53.121739    57.217499    49.327499    53.336522   \n",
       "       2018-09-30    55.582763    57.417500    53.825001    55.518421   \n",
       "       2018-10-31    55.300000    58.367500    51.522499    55.211413   \n",
       "       2018-11-30    47.954881    55.590000    42.564999    47.808929   \n",
       "       2018-12-31    41.310789    46.235001    36.647499    41.066579   \n",
       "AMZN   2018-01-31  1301.377151  1472.579956  1170.510010  1309.010946   \n",
       "       2018-02-28  1447.113159  1528.699951  1265.930054  1442.363146   \n",
       "       2018-03-31  1542.160464  1617.540039  1365.199951  1540.367629   \n",
       "       2018-04-30  1475.841902  1638.099976  1352.880005  1468.220471   \n",
       "       2018-05-31  1590.474543  1635.000000  1546.020020  1594.903637   \n",
       "       2018-06-30  1699.088582  1763.099976  1635.089966  1698.823812   \n",
       "       2018-07-31  1786.305716  1880.050049  1678.060059  1784.649042   \n",
       "       2018-08-31  1891.957833  2025.569946  1776.020020  1897.851308   \n",
       "       2018-09-30  1969.239476  2050.500000  1865.000000  1966.077900   \n",
       "       2018-10-31  1799.630865  2033.189941  1476.359985  1782.058265   \n",
       "       2018-11-30  1622.323806  1784.000000  1420.000000  1625.483823   \n",
       "       2018-12-31  1572.922100  1778.339966  1307.000000  1559.443154   \n",
       "FB     2018-01-31   184.584284   190.660004   175.800003   184.962856   \n",
       "       2018-02-28   180.721578   195.320007   167.179993   180.269473   \n",
       "       2018-03-31   173.449524   186.100006   149.020004   173.489522   \n",
       "       2018-04-30   164.163332   177.100006   150.509995   163.810476   \n",
       "       2018-05-31   181.910909   192.720001   170.229996   182.930000   \n",
       "       2018-06-30   194.974763   203.550003   186.429993   195.267620   \n",
       "       2018-07-31   199.332381   218.619995   166.559998   199.967142   \n",
       "       2018-08-31   177.598695   188.300003   170.270004   177.492172   \n",
       "       2018-09-30   164.233158   173.889999   158.869995   164.377368   \n",
       "       2018-10-31   154.873479   165.880005   139.029999   154.187826   \n",
       "       2018-11-30   141.762857   154.130005   126.849998   141.635715   \n",
       "       2018-12-31   137.529475   147.190002   123.019997   137.161052   \n",
       "GOOG   2018-01-31  1127.200945  1186.890015  1045.229980  1130.770467   \n",
       "       2018-02-28  1088.629472  1174.000000   992.559998  1088.206839   \n",
       "       2018-03-31  1096.108085  1177.050049   980.640015  1091.490479   \n",
       "       2018-04-30  1038.415237  1094.165039   990.369995  1035.696187   \n",
       "       2018-05-31  1064.021376  1110.750000  1006.289978  1069.275901   \n",
       "       2018-06-30  1136.396182  1186.286011  1096.010010  1137.626668   \n",
       "       2018-07-31  1183.464280  1273.890015  1093.800049  1187.590472   \n",
       "       2018-08-31  1226.156951  1256.500000  1188.239990  1225.671732   \n",
       "       2018-09-30  1176.878424  1212.989990  1146.910034  1175.808934   \n",
       "       2018-10-31  1116.082172  1209.959961   995.830017  1110.940411   \n",
       "       2018-11-30  1054.971424  1095.569946   996.020020  1056.162394   \n",
       "       2018-12-31  1042.619998  1124.650024   970.109985  1037.420519   \n",
       "NFLX   2018-01-31   231.269525   286.809998   195.419998   232.908096   \n",
       "       2018-02-28   270.873158   297.359985   236.110001   271.443683   \n",
       "       2018-03-31   312.712859   333.980011   275.899994   312.228097   \n",
       "       2018-04-30   309.129524   338.820007   271.220001   307.466192   \n",
       "       2018-05-31   329.779541   356.100006   305.730011   331.536819   \n",
       "       2018-06-30   384.557143   423.209991   352.820007   384.133336   \n",
       "       2018-07-31   380.969526   419.769989   328.000000   381.515238   \n",
       "       2018-08-31   345.410001   376.809998   310.929993   346.257824   \n",
       "       2018-09-30   363.326843   383.200012   335.829987   362.641576   \n",
       "       2018-10-31   340.025218   386.799988   271.209991   335.445652   \n",
       "       2018-11-30   290.643335   332.049988   250.000000   290.344764   \n",
       "       2018-12-31   266.309474   298.720001   231.229996   265.302630   \n",
       "\n",
       "                         volume  \n",
       "ticker date                      \n",
       "AAPL   2018-01-31  2.638718e+09  \n",
       "       2018-02-28  3.711577e+09  \n",
       "       2018-03-31  2.854911e+09  \n",
       "       2018-04-30  2.664617e+09  \n",
       "       2018-05-31  2.483905e+09  \n",
       "       2018-06-30  2.110498e+09  \n",
       "       2018-07-31  1.574766e+09  \n",
       "       2018-08-31  2.801276e+09  \n",
       "       2018-09-30  2.715888e+09  \n",
       "       2018-10-31  3.158994e+09  \n",
       "       2018-11-30  3.845306e+09  \n",
       "       2018-12-31  3.595690e+09  \n",
       "AMZN   2018-01-31  9.637120e+07  \n",
       "       2018-02-28  1.377840e+08  \n",
       "       2018-03-31  1.304001e+08  \n",
       "       2018-04-30  1.299196e+08  \n",
       "       2018-05-31  7.161550e+07  \n",
       "       2018-06-30  8.594130e+07  \n",
       "       2018-07-31  9.752110e+07  \n",
       "       2018-08-31  9.657580e+07  \n",
       "       2018-09-30  9.444550e+07  \n",
       "       2018-10-31  1.832208e+08  \n",
       "       2018-11-30  1.392900e+08  \n",
       "       2018-12-31  1.548127e+08  \n",
       "FB     2018-01-31  4.956557e+08  \n",
       "       2018-02-28  5.162516e+08  \n",
       "       2018-03-31  9.962017e+08  \n",
       "       2018-04-30  7.500727e+08  \n",
       "       2018-05-31  4.011441e+08  \n",
       "       2018-06-30  3.872656e+08  \n",
       "       2018-07-31  6.470307e+08  \n",
       "       2018-08-31  5.488327e+08  \n",
       "       2018-09-30  5.004688e+08  \n",
       "       2018-10-31  6.224463e+08  \n",
       "       2018-11-30  5.181517e+08  \n",
       "       2018-12-31  5.587862e+08  \n",
       "GOOG   2018-01-31  2.873840e+07  \n",
       "       2018-02-28  4.238200e+07  \n",
       "       2018-03-31  4.535330e+07  \n",
       "       2018-04-30  4.171590e+07  \n",
       "       2018-05-31  3.184940e+07  \n",
       "       2018-06-30  3.209600e+07  \n",
       "       2018-07-31  3.194010e+07  \n",
       "       2018-08-31  2.880840e+07  \n",
       "       2018-09-30  2.886240e+07  \n",
       "       2018-10-31  4.849470e+07  \n",
       "       2018-11-30  3.673510e+07  \n",
       "       2018-12-31  4.025760e+07  \n",
       "NFLX   2018-01-31  2.383776e+08  \n",
       "       2018-02-28  1.845858e+08  \n",
       "       2018-03-31  2.634494e+08  \n",
       "       2018-04-30  2.620060e+08  \n",
       "       2018-05-31  1.420508e+08  \n",
       "       2018-06-30  2.440318e+08  \n",
       "       2018-07-31  3.053938e+08  \n",
       "       2018-08-31  2.131223e+08  \n",
       "       2018-09-30  1.708321e+08  \n",
       "       2018-10-31  3.635898e+08  \n",
       "       2018-11-30  2.571264e+08  \n",
       "       2018-12-31  2.343100e+08  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import warnings package to suppress Pandas FutureWarning statements\n",
    "## FutureWarning statements about the NumPy functions\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Method chaining\n",
    "faang.set_index('date').groupby('ticker').resample('1M').agg(\n",
    "    {\n",
    "        'open': np.mean,\n",
    "        'high': np.max,\n",
    "        'low': np.min,\n",
    "        'close': np.mean,\n",
    "        'volume': np.sum\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee143c1-5151-43a8-8e8c-4efce6744ff9",
   "metadata": {},
   "source": [
    "### **Sources:**\n",
    "\n",
    "> **In-Text:**\n",
    "* Ch. 4, pp. 230-233\n",
    "    * Sections: *\"Summarizing DataFrames\"* & *\"Aggregating by Group\"*\n",
    "* Ch. 4, pp. 250-254\n",
    "    * Section: *\"Resampling\"*\n",
    "\n",
    "> **Supplemental:**\n",
    "* stackoverflow: __[AttributeError: 'DataFrame' object has no attribute 'to_datetime'](https://stackoverflow.com/questions/48387878/attributeerror-dataframe-object-has-no-attribute-to-datetime)__\n",
    "* pandas.pydata: __[pandas.read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)__\n",
    "* Medium: __[Module 3: Handling Time Series Data Topic: Resampling and Aggregating Time Series Data](https://medium.com/@sujathamudadla1213/module-3-handling-time-series-data-topic-resampling-and-aggregating-time-series-data-917821641077)__\n",
    "* Towards Data Science: __[Resample function of Pandas](https://towardsdatascience.com/resample-function-of-pandas-79b17ec82a78)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86356bf1-75d3-4fdf-8767-a9f659f5a345",
   "metadata": {},
   "source": [
    "### **Notes:**\n",
    "> On **Method Chaining:**\n",
    "> * Calls are chained together in a single statement, which returns a single object instead of a series of intermediate results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b6faa3-859d-4e87-bc5e-26d8a738049c",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# <font color=blue>**4.**</font> **Aggregation Formats:** <center> Crosstabs\n",
    "\n",
    "> <font color=deeppink>**Build a crosstab** with: <br>\n",
    "> The `earthquake` data between the</font> **tsunami** <font color=deeppink>column and the</font> **magType** <font color=deeppink>column.\n",
    "\n",
    "> <font color=deeppink>Rather than showing the frequency count, **show** the **maximum** magnitude that was observed for *each combination.*\n",
    "> > <font color=deeppink>Put the magnitude type *along the columns*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bcae5e4-72fb-410b-b276-94394866296b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>magType</th>\n",
       "      <th>mb</th>\n",
       "      <th>mb_lg</th>\n",
       "      <th>md</th>\n",
       "      <th>mh</th>\n",
       "      <th>ml</th>\n",
       "      <th>ms_20</th>\n",
       "      <th>mw</th>\n",
       "      <th>mwb</th>\n",
       "      <th>mwr</th>\n",
       "      <th>mww</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tsunami</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.6</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.11</td>\n",
       "      <td>1.1</td>\n",
       "      <td>4.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.83</td>\n",
       "      <td>5.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.1</td>\n",
       "      <td>5.7</td>\n",
       "      <td>4.41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "magType   mb  mb_lg    md   mh   ml  ms_20    mw  mwb  mwr  mww\n",
       "tsunami                                                        \n",
       "0        5.6    3.5  4.11  1.1  4.2    NaN  3.83  5.8  4.8  6.0\n",
       "1        6.1    NaN   NaN  NaN  5.1    5.7  4.41  NaN  NaN  7.5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crosstabs are a fast way of formatting aggregated data\n",
    "## Not as customizable as .groupby(), .resample(), or other aggregation methods\n",
    "\n",
    "pd.crosstab(earthquakes.tsunami,   # Index: values to group by in the rows\n",
    "            earthquakes.magType,   # Columns: values to group by in the columns\n",
    "            values=earthquakes.mag, # Array of values to aggregate according to\n",
    "            aggfunc='max'           # `aggfunc` parameter required to use `values`\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0e5f92-0cd4-4a55-b300-bdb057b92f08",
   "metadata": {},
   "source": [
    "**NOTE:** From pandas.pydata on `pandas.crosstab()` syntax: <br>\n",
    "**Syntax:** <br>\n",
    "> `pandas.crosstab`(***index**, **columns**, **values**=None, **rownames**=None, **colnames**=None, **aggfunc**=None, **margins**=False, **margins_name**='ALL', **dropna**=True, **normalize**=False*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53db9cbd-099b-4cc6-85d0-f5dac2eec1bc",
   "metadata": {},
   "source": [
    "### **Sources:**\n",
    "\n",
    "> **In-Text:**\n",
    "* Ch. 4, pp. 237-241\n",
    "    * Sections: *\"Aggregating data* > *\"Pivot tables and crosstabs\"*\n",
    "\n",
    "> **Supplemental:**\n",
    "* Medium: __[The Power of Crosstab Function in Pandas for Data Analysis and Visualization](https://medium.com/geekculture/the-power-of-crosstab-function-in-pandas-for-data-analysis-and-visualization-6c085c269fcd)__\n",
    "* pandas.pydata: __[pandas.crosstab()](https://pandas.pydata.org/docs/reference/api/pandas.crosstab.html)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76377ac6-75d5-4809-a15a-4188ea025820",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# <font color=blue>**5.**</font> **Window Calculations:** <center> Rolling Windows\n",
    "\n",
    "> <font color=deeppink>**Calculate** the **rolling 60-day** (`.rolling('60D')`) aggregations of the OHLC data by ticker for the FAANG data.\n",
    "> > <font color=deeppink>Use the same **aggregations** as exercise <font color=blue>**3**:</font>\n",
    "> > * <font color=deeppink>**Mean** of the opening price\n",
    "> > * <font color=deeppink>**Maximum** of the high price\n",
    "> > * <font color=deeppink>**Minimum** of the low price\n",
    "> > * <font color=deeppink>**Mean** of the closing price\n",
    "> > * <font color=deeppink>**Sum** of the volume traded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12242862-5582-4d13-8321-9f2f7ec47a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FB</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>181.580002</td>\n",
       "      <td>177.550003</td>\n",
       "      <td>177.679993</td>\n",
       "      <td>181.419998</td>\n",
       "      <td>18151900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FB</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>184.779999</td>\n",
       "      <td>181.330002</td>\n",
       "      <td>181.880005</td>\n",
       "      <td>184.669998</td>\n",
       "      <td>16886600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FB</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>186.210007</td>\n",
       "      <td>184.100006</td>\n",
       "      <td>184.899994</td>\n",
       "      <td>184.330002</td>\n",
       "      <td>13880900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FB</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>186.899994</td>\n",
       "      <td>184.929993</td>\n",
       "      <td>185.589996</td>\n",
       "      <td>186.850006</td>\n",
       "      <td>13574500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FB</td>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>188.899994</td>\n",
       "      <td>186.330002</td>\n",
       "      <td>187.199997</td>\n",
       "      <td>188.279999</td>\n",
       "      <td>17994700.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker       date        high         low        open       close  \\\n",
       "0     FB 2018-01-02  181.580002  177.550003  177.679993  181.419998   \n",
       "1     FB 2018-01-03  184.779999  181.330002  181.880005  184.669998   \n",
       "2     FB 2018-01-04  186.210007  184.100006  184.899994  184.330002   \n",
       "3     FB 2018-01-05  186.899994  184.929993  185.589996  186.850006   \n",
       "4     FB 2018-01-08  188.899994  186.330002  187.199997  188.279999   \n",
       "\n",
       "       volume  \n",
       "0  18151900.0  \n",
       "1  16886600.0  \n",
       "2  13880900.0  \n",
       "3  13574500.0  \n",
       "4  17994700.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "window must be an integer 0 or greater",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m display(faang\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Method chaining again\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m faang\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mticker\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mrolling(window\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m60D\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39magg(\n\u001b[0;32m      9\u001b[0m     {\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopen\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean,\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhigh\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmax,\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlow\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmin,\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean,\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvolume\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39msum\n\u001b[0;32m     15\u001b[0m     }\n\u001b[0;32m     16\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:3748\u001b[0m, in \u001b[0;36mGroupBy.rolling\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3618\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3619\u001b[0m \u001b[38;5;124;03mReturn a rolling grouper, providing rolling functionality per group.\u001b[39;00m\n\u001b[0;32m   3620\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3744\u001b[0m \u001b[38;5;124;03m  3  4  0.705\u001b[39;00m\n\u001b[0;32m   3745\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3746\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwindow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RollingGroupby\n\u001b[1;32m-> 3748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m RollingGroupby(\n\u001b[0;32m   3749\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj,\n\u001b[0;32m   3750\u001b[0m     \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m   3751\u001b[0m     _grouper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrouper,\n\u001b[0;32m   3752\u001b[0m     _as_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_index,\n\u001b[0;32m   3753\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3754\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\window\\rolling.py:711\u001b[0m, in \u001b[0;36mBaseWindowGroupby.__init__\u001b[1;34m(self, obj, _grouper, _as_index, *args, **kwargs)\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    710\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep not implemented for groupby\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 711\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(obj, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\window\\rolling.py:169\u001b[0m, in \u001b[0;36mBaseWindow.__init__\u001b[1;34m(self, obj, window, min_periods, center, win_type, axis, on, closed, step, method, selection)\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid on specified as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust be a column (of DataFrame), an Index or None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    166\u001b[0m     )\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selection \u001b[38;5;241m=\u001b[39m selection\n\u001b[1;32m--> 169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\window\\rolling.py:1908\u001b[0m, in \u001b[0;36mRolling._validate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1906\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1907\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_integer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1908\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow must be an integer 0 or greater\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: window must be an integer 0 or greater"
     ]
    }
   ],
   "source": [
    "# Suppress FutureWarning statements about the NumPy functions again\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Show the FAANG data again for reference\n",
    "display(faang.head())\n",
    "\n",
    "# Method chaining again\n",
    "faang.groupby('ticker').rolling(window='60D').agg(\n",
    "    {\n",
    "        'open': np.mean,\n",
    "        'high': np.max,\n",
    "        'low': np.min,\n",
    "        'close': np.mean,\n",
    "        'volume': np.sum\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9f2972-13ee-4528-8859-56ee9ff5ead4",
   "metadata": {},
   "source": [
    "### **Sources:**\n",
    "\n",
    "> **In-Text:**\n",
    "* Ch. 4, pp. 220-22\n",
    "    * Sections: *\"Window Calculations\"* > *\"Rolling Widows\"*\n",
    "\n",
    "> **Supplemental:**\n",
    "* pandas.pydata: __[Windowing operations](https://pandas.pydata.org/docs/user_guide/window.html#window-generic)__\n",
    "* stackoverflow: __[ValueError: window must be an integer 0 or greater when using rolling window on reset_index)](https://stackoverflow.com/questions/78222050/valueerror-window-must-be-an-integer-0-or-greater-when-using-rolling-window-on)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b88957-1433-47a5-b336-b66b9b631186",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# <font color=blue>**6.**</font> **Aggregation Data:** <center> Summarizing Data with Pivot Tables\n",
    "\n",
    "> <font color=deeppink>**Create** a pivot table of the FAANG data that **compares** the stocks.\n",
    "> > <font color=deeppink>**Put** the *ticker in the rows* and **show** the **averages** (mean) of the OHLC and volume traded data.\n",
    "\n",
    "**NOTES:**\n",
    "* \"... put the ticker in the rows\" = **Index:** values to group by in the rows\n",
    "* In `.pivot_table()` function, the *aggfunc=* parameter default is to calculate **mean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe369e9-a30d-4904-8180-ce2d36315a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot tables are another common format for aggregating data\n",
    "\n",
    "# Group the tickers in the rows\n",
    "## Easily compare their stock values from right-to-left\n",
    "\n",
    "# aggfunc= parameter does not need to be specified since default is mean calculation\n",
    "faang.pivot_table(index='ticker')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4f30f3-256d-4304-86c9-42ccfe2bdde8",
   "metadata": {},
   "source": [
    "### **Sources:**\n",
    "\n",
    "> **In-Text:**\n",
    "* Ch. 4, pp. 237-241\n",
    "    * Sections: *\"Aggregating data\"* > *\"Rolling Widows\"*\n",
    "\n",
    "> **Supplemental:**\n",
    "* pandas.pydata: __[pandas.pivot_table()](https://pandas.pydata.org/docs/reference/api/pandas.pivot_table.html)__\n",
    "* pandas.pydata: __[Reshaping and pivot tables](https://pandas.pydata.org/docs/user_guide/reshaping.html)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607342be-f513-429c-8489-9e81d0136f93",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# <font color=blue>**7.**</font> **Applying Functions:** <center> Using DataFrame Operations to enrich data\n",
    "\n",
    "> <font color=deeppink>**Calculate** the Z-scores for each numeric column of Amazon's data (`ticker` is AMZN) in Q4 2018 using the `apply()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b38de955-07f9-4172-b48e-b4631a75e927",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'2018-Q4'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m##### Running into a KeyError and I think it has to do with the issue I'm having \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m##### in Exercise 5. Perhaps it is not aggregating the Quarters there?\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Method chaining again\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# lambda functions can be applied to rows and columns in a DataFrame\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m## Using `.apply()' enables clean method chaining (Pythonic)\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m faang\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2018-Q4\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mticker == \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAMZ\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mticker\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39msub(x\u001b[38;5;241m.\u001b[39mmean())\u001b[38;5;241m.\u001b[39mdiv(x\u001b[38;5;241m.\u001b[39mstd())\n\u001b[0;32m     10\u001b[0m )\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1153\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1150\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1152\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m-> 1153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_axis(maybe_callable, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1393\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;66;03m# fall thru to straight lookup\u001b[39;00m\n\u001b[0;32m   1392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m-> 1393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_label(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1343\u001b[0m, in \u001b[0;36m_LocIndexer._get_label\u001b[1;34m(self, label, axis)\u001b[0m\n\u001b[0;32m   1341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_label\u001b[39m(\u001b[38;5;28mself\u001b[39m, label, axis: AxisInt):\n\u001b[0;32m   1342\u001b[0m     \u001b[38;5;66;03m# GH#5567 this will fail if the label is not present in the axis.\u001b[39;00m\n\u001b[1;32m-> 1343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mxs(label, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4236\u001b[0m, in \u001b[0;36mNDFrame.xs\u001b[1;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[0;32m   4234\u001b[0m             new_index \u001b[38;5;241m=\u001b[39m index[loc]\n\u001b[0;32m   4235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4236\u001b[0m     loc \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loc, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m   4239\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m loc\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mbool_:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:418\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[1;32m--> 418\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: '2018-Q4'"
     ]
    }
   ],
   "source": [
    "##### Running into a KeyError and I think it has to do with the issue I'm having \n",
    "##### in Exercise 5. Perhaps it is not aggregating the Quarters there?\n",
    "\n",
    "# Method chaining again\n",
    "# lambda functions can be applied to rows and columns in a DataFrame\n",
    "## Using `.apply()' enables clean method chaining (Pythonic)\n",
    "\n",
    "faang.loc['2018-Q4'].query(\"ticker == 'AMZ'\").drop(columns='ticker').apply(\n",
    "    lambda x: x.sub(x.mean()).div(x.std())\n",
    ").head() # Display the head of the Z-score DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d153c5ee-701e-4525-8d81-ee62ff22af33",
   "metadata": {},
   "source": [
    "### **Sources:**\n",
    "\n",
    "> **In-Text:**\n",
    "* Ch. 4, pp. 217-220\n",
    "    * Sections: *\"Using DataFrame operations to enrich data\"* > *\"Applying functions\"*\n",
    "\n",
    "> **Supplemental:**\n",
    "* GeeksforGeek: __[Applying Lambda functions to Pandas Dataframe()](https://www.geeksforgeeks.org/applying-lambda-functions-to-pandas-dataframe/)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affbc7aa-1817-4aaa-ac9f-89a9f7bd1438",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# <font color=blue>**8.**</font> **Combining DataFrames:** <center> Joins\n",
    "\n",
    "> <font color=deeppink>**Add** event descriptions:\n",
    "> > <font color=deeppink>**Create a dataframe** with the following three(3) columns:\n",
    "> > > `Ticker`, `date`, and `event`\n",
    "> >\n",
    "> > <font color=deeppink>The columns should have the following values:\n",
    "> > > * `Ticker: 'FB'`\n",
    "> > > * `Date: ['2018-07-25', '2018-03-19', '2018-03-20']`\n",
    "> > > * `Event: ['Disappointing user growth announced ater close.', 'Cambridge Analytica story', 'FTC investigation']`\n",
    "\n",
    ">  <font color=deeppink>**Set the index** to `['date', 'ticker']`.\n",
    "\n",
    "> <font color=deeppink>**Merge** this data with the FAANG data using an **outer join.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4e122d-65d0-4f51-9bab-1539a42f4afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an entirely new DataFrame\n",
    "## Add in the specified events\n",
    "events = pd.DataFrame({   # Create a dictionary to input the event data\n",
    "    'ticker': 'FB',\n",
    "    'date': pd.to_datetime(\n",
    "        ['2018-07-25', '2018-03-19', '2018-03-20']\n",
    "    ),\n",
    "    'event': [\n",
    "        'Disappointing user growth announced after close.',\n",
    "        'Cambridge Analytica story',\n",
    "        'FTC investigation'\n",
    "    ]\n",
    "}).set_index(['date', 'ticker'])   # Specify the index for the new 'event' DF\n",
    "\n",
    "\n",
    "# Reset index to undo all the previous index modifications to original dataframe\n",
    "## Using `.join()` combines the original FAANG DF with the new 'events' DF\n",
    "\n",
    "faang.reset_index().set_index(['date', 'ticker']).join(\n",
    "    events, how='outer'    # This is a full Outer join = returns all rows of both DFs\n",
    ").sample(10, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3fe5d8-665d-473d-9a3f-4c7ff51e078c",
   "metadata": {},
   "source": [
    "### **Sources:**\n",
    "\n",
    "> **In-Text:**\n",
    "* Ch. 4, pp. 198-209\n",
    "    * Sections: *\"Merging DataFrames\"*\n",
    "\n",
    "> **Supplemental:**\n",
    "* GeeksforGeek: __[Different Types of Joins in Pandas](https://www.geeksforgeeks.org/different-types-of-joins-in-pandas//)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066a3928-fd31-47ce-8f59-5318a659932e",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# <font color=blue>**9.**</font> **Aggregating DataFrames:** <center> Transformations\n",
    "\n",
    "> <font color=deeppink>**Use the `transform()` method** on the FAANG data to represent all the values in terms of the first date in the data.\n",
    "> > <font color=deeppink>To do so, **divide** all the values for each ticker by the values for the first date in the data for that ticker. <br>\n",
    "> > This is referred to as an ***index***, and the data for the first date is the ***base***.<center> __[Beginners: Statistical concept - Index and base year](https://ec.europa.eu/eurostat/statistics-explained/index.php?title=Beginners:Statistical_concept_-_Index_and_base_year)__\n",
    "> > > <font color=deeppink>When data is in this format, we can easily see growth over time.<br> <center><font color=#dc143c>**Hint:**</font> `transform()` can take a function name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4f0e4b0-c3ab-4356-af93-714a9cddba2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">FB</th>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>inf</td>\n",
       "      <td>1.017623</td>\n",
       "      <td>1.021290</td>\n",
       "      <td>1.023638</td>\n",
       "      <td>1.017914</td>\n",
       "      <td>0.930294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>inf</td>\n",
       "      <td>1.025498</td>\n",
       "      <td>1.036891</td>\n",
       "      <td>1.040635</td>\n",
       "      <td>1.016040</td>\n",
       "      <td>0.764708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">AAPL</th>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>1.003984</td>\n",
       "      <td>1.013059</td>\n",
       "      <td>1.015952</td>\n",
       "      <td>1.013928</td>\n",
       "      <td>0.999826</td>\n",
       "      <td>1.155033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>1.007968</td>\n",
       "      <td>1.006790</td>\n",
       "      <td>1.016661</td>\n",
       "      <td>1.013987</td>\n",
       "      <td>1.004470</td>\n",
       "      <td>0.877864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">AMZN</th>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>1.001992</td>\n",
       "      <td>1.013017</td>\n",
       "      <td>1.015199</td>\n",
       "      <td>1.013908</td>\n",
       "      <td>1.012775</td>\n",
       "      <td>1.153758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>1.003984</td>\n",
       "      <td>1.021739</td>\n",
       "      <td>1.029175</td>\n",
       "      <td>1.028157</td>\n",
       "      <td>1.017308</td>\n",
       "      <td>1.121581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">NFLX</th>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>1.001328</td>\n",
       "      <td>1.022614</td>\n",
       "      <td>1.031112</td>\n",
       "      <td>1.030342</td>\n",
       "      <td>1.019794</td>\n",
       "      <td>0.783394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>1.002656</td>\n",
       "      <td>1.026779</td>\n",
       "      <td>1.043905</td>\n",
       "      <td>1.051504</td>\n",
       "      <td>1.022679</td>\n",
       "      <td>0.549800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">GOOG</th>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>1.000996</td>\n",
       "      <td>1.018136</td>\n",
       "      <td>1.017202</td>\n",
       "      <td>1.015234</td>\n",
       "      <td>1.016413</td>\n",
       "      <td>1.155624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>1.001992</td>\n",
       "      <td>1.024959</td>\n",
       "      <td>1.037094</td>\n",
       "      <td>1.037831</td>\n",
       "      <td>1.020094</td>\n",
       "      <td>0.811732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      index      high       low      open     close    volume\n",
       "ticker date                                                                  \n",
       "FB     2018-01-02       NaN  1.000000  1.000000  1.000000  1.000000  1.000000\n",
       "       2018-01-03       inf  1.017623  1.021290  1.023638  1.017914  0.930294\n",
       "       2018-01-04       inf  1.025498  1.036891  1.040635  1.016040  0.764708\n",
       "AAPL   2018-01-02  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000\n",
       "       2018-01-03  1.003984  1.013059  1.015952  1.013928  0.999826  1.155033\n",
       "       2018-01-04  1.007968  1.006790  1.016661  1.013987  1.004470  0.877864\n",
       "AMZN   2018-01-02  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000\n",
       "       2018-01-03  1.001992  1.013017  1.015199  1.013908  1.012775  1.153758\n",
       "       2018-01-04  1.003984  1.021739  1.029175  1.028157  1.017308  1.121581\n",
       "NFLX   2018-01-02  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000\n",
       "       2018-01-03  1.001328  1.022614  1.031112  1.030342  1.019794  0.783394\n",
       "       2018-01-04  1.002656  1.026779  1.043905  1.051504  1.022679  0.549800\n",
       "GOOG   2018-01-02  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000\n",
       "       2018-01-03  1.000996  1.018136  1.017202  1.015234  1.016413  1.155624\n",
       "       2018-01-04  1.001992  1.024959  1.037094  1.037831  1.020094  0.811732"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `transform()` is often combined with `groupby()`\n",
    "\n",
    "# faang_index is the function name for .transform()\n",
    "## faang_index performs calculations on the values of each ticker\n",
    "faang = faang.reset_index().set_index(['ticker', 'date'])\n",
    "faang_index = (faang / faang.groupby(level='ticker').transform('first')) #func='first'\n",
    "\n",
    "# Aggregating the transformed DF faang_index\n",
    "# .agg() will show the first 3 rows of each ticker when axis= parameter is specified\n",
    "faang_index.groupby(level='ticker').agg('head', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7511b5e-0588-4de8-912e-7245ebe3e681",
   "metadata": {},
   "source": [
    "### **Sources:**\n",
    "\n",
    "> **In-Text:**\n",
    "* Ch. 4, pp. 236-237\n",
    "    * Sections: *\"Aggregating by group\"*\n",
    "\n",
    "> **Supplemental:**\n",
    "* Towards Data Science: __[When to use Pandas transform() function](https://towardsdatascience.com/when-to-use-pandas-transform-function-df8861aa0dcf)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca5e45e-040f-4a59-b9b5-e34796c9bfaf",
   "metadata": {},
   "source": [
    "---\n",
    "___\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
